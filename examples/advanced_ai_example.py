"""
Exemplo de Uso dos Modelos de IA Avan√ßados
Vers√£o 1.5 - IA Avan√ßada e Escalabilidade

Demonstra como usar os modelos de IA avan√ßados:
- Transformers para an√°lise de sentimento e pol√≠ticas
- LSTM para previs√£o de s√©ries temporais
- GANs para gera√ß√£o de dados sint√©ticos
- Reinforcement Learning para tomada de decis√µes
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch  # noqa: E402
import numpy as np  # noqa: E402
import matplotlib.pyplot as plt  # noqa: E402

# datetime removido - n√£o utilizado
from typing import List  # noqa: E402

from src.ai.advanced_models import AdvancedAIManager  # noqa: E402
from src.ai.advanced_models.transformer_models import (  # noqa: E402
    CityTransformerManager,
)
from src.ai.advanced_models.lstm_models import LSTMModelManager  # noqa: E402
from src.ai.advanced_models.gan_models import GANModelManager  # noqa: E402
from src.ai.advanced_models.reinforcement_learning import MultiAgentRL  # noqa: E402


class CitySimulationEnvironment:
    """Ambiente de simula√ß√£o simples para demonstra√ß√£o"""

    def __init__(self, num_agents: int = 4):
        self.num_agents = num_agents
        self.state_size = 10
        self.action_size = 5
        self.current_states = np.random.randn(num_agents, self.state_size)
        self.step_count = 0
        self.max_steps = 100

    def reset(self) -> List[np.ndarray]:
        """Reset do ambiente"""
        self.current_states = np.random.randn(self.num_agents, self.state_size)
        self.step_count = 0
        return [self.current_states[i] for i in range(self.num_agents)]

    def step(self, actions: List[int]) -> tuple:
        """Executa a√ß√µes e retorna pr√≥ximo estado"""
        next_states = []
        rewards = []
        dones = []

        for i, action in enumerate(actions):
            # Simular transi√ß√£o de estado
            noise = np.random.randn(self.state_size) * 0.1
            next_state = self.current_states[i] + noise + action * 0.1
            next_states.append(next_state)

            # Simular recompensa
            reward = np.random.randn() + action * 0.5
            rewards.append(reward)

            # Simular t√©rmino
            done = self.step_count >= self.max_steps
            dones.append(done)

        self.current_states = np.array(next_states)
        self.step_count += 1

        return next_states, rewards, dones, {}


def demonstrate_transformers():
    """Demonstra uso dos modelos Transformer"""
    print("=== DEMONSTRA√á√ÉO DE TRANSFORMERS ===")

    # Inicializar gerenciador
    manager = CityTransformerManager(device="cpu")

    # Dados de exemplo para an√°lise de sentimento
    sentiment_data = torch.randint(0, 1000, (100, 50))  # 100 amostras, 50 tokens
    sentiment_labels = torch.randint(0, 3, (100,))  # 3 classes: positivo, neutro, negativo

    # Criar DataLoader simples
    from torch.utils.data import DataLoader, TensorDataset

    dataset = TensorDataset(sentiment_data, sentiment_labels)
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

    # Treinar modelo de sentimento
    print("Treinando modelo de an√°lise de sentimento...")
    losses = manager.train_model("sentiment", dataloader, epochs=10)
    print(f"Losses finais: {losses[-5:]}")

    # Fazer predi√ß√£o
    test_data = torch.randint(0, 1000, (5, 50))
    predictions = manager.predict("sentiment", test_data)
    print(f"Predi√ß√µes de sentimento: {predictions.argmax(dim=1)}")

    # Informa√ß√µes do modelo
    info = manager.get_model_info()
    print(f"Informa√ß√µes do modelo: {info['sentiment']}")


def demonstrate_lstm():
    """Demonstra uso dos modelos LSTM"""
    print("\n=== DEMONSTRA√á√ÉO DE LSTM ===")

    # Inicializar gerenciador
    manager = LSTMModelManager(device="cpu")

    # Dados de exemplo para previs√£o de tr√°fego
    sequence_length = 24  # 24 horas
    num_features = 10
    num_samples = 200

    # Gerar dados sint√©ticos de tr√°fego
    traffic_data = np.random.randn(num_samples, sequence_length, num_features)
    traffic_targets = np.random.randn(num_samples, 24)  # Previs√£o para pr√≥ximas 24 horas

    # Criar DataLoader
    from torch.utils.data import DataLoader, TensorDataset

    dataset = TensorDataset(torch.FloatTensor(traffic_data), torch.FloatTensor(traffic_targets))
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    # Treinar modelo de tr√°fego
    print("Treinando modelo de previs√£o de tr√°fego...")
    losses = manager.train_model("traffic", dataloader, epochs=20)
    print(f"Losses finais: {losses[-5:]}")

    # Fazer predi√ß√£o
    test_data = torch.FloatTensor(np.random.randn(5, sequence_length, num_features))
    spatial_data = torch.FloatTensor(np.random.randn(5, sequence_length, num_features))

    predictions = manager.predict("traffic", test_data, spatial_data)
    print(f"Predi√ß√µes de tr√°fego shape: {predictions['traffic_prediction'].shape}")
    print(f"Confian√ßa shape: {predictions['confidence'].shape}")

    # Informa√ß√µes do modelo
    info = manager.get_model_info()
    print(f"Informa√ß√µes do modelo: {info['traffic']}")


def demonstrate_gans():
    """Demonstra uso dos modelos GAN"""
    print("\n=== DEMONSTRA√á√ÉO DE GANs ===")

    # Inicializar gerenciador
    manager = GANModelManager(device="cpu")

    # Dados de exemplo para treinamento
    real_data = torch.randn(1000, 64)  # 1000 amostras, 64 caracter√≠sticas

    # Criar DataLoader
    from torch.utils.data import DataLoader, TensorDataset

    dataset = TensorDataset(real_data)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    # Treinar GAN para dados de cidade
    print("Treinando GAN para dados de cidade...")
    losses = manager.train_model("city_data", dataloader, epochs=50)
    print(f"Losses finais - D: {losses['d_losses'][-5:]}, G: {losses['g_losses'][-5:]}")

    # Gerar dados sint√©ticos
    synthetic_data = manager.generate_data("city_data", num_samples=100)
    print(f"Dados sint√©ticos gerados: {synthetic_data.shape}")

    # Gerar agentes sint√©ticos
    agents = manager.models["agents"].generate_agent(num_agents=10, device="cpu")
    print(f"Agentes sint√©ticos gerados: {agents['agent_data'].shape}")
    print(f"Tipos de agentes: {agents['agent_types'].argmax(dim=1)}")

    # Informa√ß√µes do modelo
    info = manager.get_model_info()
    print(f"Informa√ß√µes do modelo: {info['city_data']}")


def demonstrate_reinforcement_learning():
    """Demonstra uso do Reinforcement Learning"""
    print("\n=== DEMONSTRA√á√ÉO DE REINFORCEMENT LEARNING ===")

    # Criar ambiente de simula√ß√£o
    environment = CitySimulationEnvironment(num_agents=4)

    # Inicializar sistema RL
    rl_system = MultiAgentRL(num_agents=4, state_size=10, action_size=5, algorithm="dqn", device="cpu")

    # Treinar sistema RL
    print("Treinando sistema RL multi-agente...")
    losses = {}

    for episode in range(100):
        states = environment.reset()
        episode_rewards = [0.0] * 4

        for step in range(50):
            # Agentes agem
            actions = rl_system.act(states, training=True)

            # Ambiente responde
            next_states, rewards, dones, info = environment.step(actions)

            # Armazenar experi√™ncias
            for i, (state, action, reward, next_state, done) in enumerate(
                zip(states, actions, rewards, next_states, dones)
            ):
                rl_system.store_experience(i, state, action, reward, next_state, done)
                episode_rewards[i] += reward

            states = next_states

            if any(dones):
                break

        # Atualizar agentes a cada 10 epis√≥dios
        if episode % 10 == 0:
            episode_losses = rl_system.update_agents()
            for key, value in episode_losses.items():
                if key not in losses:
                    losses[key] = []
                losses[key].extend(value)

        if episode % 20 == 0:
            avg_reward = sum(episode_rewards) / len(episode_rewards)
            print(f"Epis√≥dio {episode}, Recompensa M√©dia: {avg_reward:.2f}")

    # Testar sistema treinado
    print("Testando sistema RL treinado...")
    test_states = environment.reset()
    test_actions = rl_system.act(test_states, training=False)
    print(f"A√ß√µes dos agentes: {test_actions}")

    # Informa√ß√µes do sistema
    info = rl_system.get_agent_info()
    print(f"Informa√ß√µes do sistema RL: {len(info)} agentes")


def demonstrate_integrated_system():
    """Demonstra sistema integrado de IA avan√ßada"""
    print("\n=== DEMONSTRA√á√ÉO DE SISTEMA INTEGRADO ===")

    # Inicializar gerenciador principal
    ai_manager = AdvancedAIManager(device="cpu")

    # Inicializar sistema RL
    ai_manager.initialize_rl_system(num_agents=4, state_size=10, action_size=5, algorithm="dqn")

    # Dados de exemplo
    sentiment_data = torch.randint(0, 1000, (50, 30))
    sentiment_labels = torch.randint(0, 3, (50,))

    traffic_data = torch.FloatTensor(np.random.randn(50, 24, 8))
    traffic_targets = torch.FloatTensor(np.random.randn(50, 24))

    # Criar DataLoaders
    from torch.utils.data import DataLoader, TensorDataset

    sentiment_dataset = TensorDataset(sentiment_data, sentiment_labels)
    sentiment_dataloader = DataLoader(sentiment_dataset, batch_size=16, shuffle=True)

    traffic_dataset = TensorDataset(traffic_data, traffic_targets)
    traffic_dataloader = DataLoader(traffic_dataset, batch_size=16, shuffle=True)

    # Treinar modelos
    print("Treinando modelos integrados...")

    # Transformer
    transformer_losses = ai_manager.train_transformer_model("sentiment", sentiment_dataloader, epochs=5)
    print(f"Transformer losses: {transformer_losses[-3:]}")

    # LSTM
    lstm_losses = ai_manager.train_lstm_model("traffic", traffic_dataloader, epochs=10)
    print(f"LSTM losses: {lstm_losses[-3:]}")

    # GAN
    real_data = torch.randn(200, 32)
    gan_dataset = TensorDataset(real_data)
    gan_dataloader = DataLoader(gan_dataset, batch_size=16, shuffle=True)
    gan_losses = ai_manager.train_gan_model("city_data", gan_dataloader, epochs=20)
    print(f"GAN losses - D: {gan_losses['d_losses'][-3:]}, G: {gan_losses['g_losses'][-3:]}")

    # RL
    environment = CitySimulationEnvironment(num_agents=4)
    rl_losses = ai_manager.train_rl_system(environment, episodes=50)
    print(f"RL losses: {len(rl_losses)} tipos de perda")

    # Fazer predi√ß√µes integradas
    print("\nFazendo predi√ß√µes integradas...")

    # An√°lise de sentimento
    sentiment_input = torch.randint(0, 1000, (3, 30))
    sentiment_pred = ai_manager.predict_with_transformer("sentiment", sentiment_input)
    print(f"Predi√ß√µes de sentimento: {sentiment_pred.argmax(dim=1)}")

    # Previs√£o de tr√°fego
    traffic_input = torch.FloatTensor(np.random.randn(3, 24, 8))
    traffic_pred = ai_manager.predict_with_lstm("traffic", traffic_input)
    print(f"Predi√ß√µes de tr√°fego shape: {traffic_pred['traffic_prediction'].shape}")

    # Gera√ß√£o de dados
    synthetic_data = ai_manager.generate_with_gan("city_data", num_samples=10)
    print(f"Dados sint√©ticos gerados: {synthetic_data.shape}")

    # A√ß√µes RL
    rl_states = [np.random.randn(10) for _ in range(4)]
    rl_actions = ai_manager.act_with_rl(rl_states, training=False)
    print(f"A√ß√µes RL: {rl_actions}")

    # Resumo de performance
    summary = ai_manager.get_performance_summary()
    print(f"\nResumo de performance: {summary}")

    # Salvar todos os modelos
    ai_manager.save_all_models("saved_models")
    print("Modelos salvos em 'saved_models'")


def create_visualization():
    """Cria visualiza√ß√µes dos resultados"""
    print("\n=== CRIANDO VISUALIZA√á√ïES ===")

    # Exemplo de visualiza√ß√£o de perdas de treinamento
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))

    # Simular dados de perda
    episodes = range(50)

    # Transformer losses
    transformer_losses = np.exp(-np.array(episodes) * 0.1) + 0.1
    axes[0, 0].plot(episodes, transformer_losses)
    axes[0, 0].set_title("Transformer Training Loss")
    axes[0, 0].set_xlabel("Epoch")
    axes[0, 0].set_ylabel("Loss")

    # LSTM losses
    lstm_losses = np.exp(-np.array(episodes) * 0.05) + 0.2
    axes[0, 1].plot(episodes, lstm_losses)
    axes[0, 1].set_title("LSTM Training Loss")
    axes[0, 1].set_xlabel("Epoch")
    axes[0, 1].set_ylabel("Loss")

    # GAN losses
    gan_d_losses = np.exp(-np.array(episodes) * 0.02) + 0.5
    gan_g_losses = np.exp(-np.array(episodes) * 0.03) + 0.3
    axes[1, 0].plot(episodes, gan_d_losses, label="Discriminator")
    axes[1, 0].plot(episodes, gan_g_losses, label="Generator")
    axes[1, 0].set_title("GAN Training Losses")
    axes[1, 0].set_xlabel("Epoch")
    axes[1, 0].set_ylabel("Loss")
    axes[1, 0].legend()

    # RL rewards
    rl_rewards = np.cumsum(np.random.randn(50) * 0.1 + 0.5)
    axes[1, 1].plot(episodes, rl_rewards)
    axes[1, 1].set_title("RL Cumulative Rewards")
    axes[1, 1].set_xlabel("Episode")
    axes[1, 1].set_ylabel("Cumulative Reward")

    plt.tight_layout()
    plt.savefig("advanced_ai_training_results.png", dpi=300, bbox_inches="tight")
    print("Visualiza√ß√£o salva como 'advanced_ai_training_results.png'")

    # Mostrar gr√°fico
    plt.show()


def main():
    """Fun√ß√£o principal de demonstra√ß√£o"""
    print("üöÄ DEMONSTRA√á√ÉO DE MODELOS DE IA AVAN√áADOS - VERS√ÉO 1.5")
    print("=" * 60)

    try:
        # Demonstrar cada tipo de modelo
        demonstrate_transformers()
        demonstrate_lstm()
        demonstrate_gans()
        demonstrate_reinforcement_learning()

        # Demonstrar sistema integrado
        demonstrate_integrated_system()

        # Criar visualiza√ß√µes
        create_visualization()

        print("\n‚úÖ Demonstra√ß√£o conclu√≠da com sucesso!")
        print("\nFuncionalidades implementadas:")
        print("- ‚úÖ Modelos Transformer para an√°lise de linguagem natural")
        print("- ‚úÖ Modelos LSTM para previs√£o de s√©ries temporais")
        print("- ‚úÖ Modelos GAN para gera√ß√£o de dados sint√©ticos")
        print("- ‚úÖ Sistema de Reinforcement Learning multi-agente")
        print("- ‚úÖ Gerenciador integrado de todos os modelos")
        print("- ‚úÖ Sistema de treinamento e avalia√ß√£o")
        print("- ‚úÖ Visualiza√ß√µes e m√©tricas de performance")

    except Exception as e:
        print(f"‚ùå Erro durante demonstra√ß√£o: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
